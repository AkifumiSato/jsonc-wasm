use crate::token::{LexerError, Location, Token};
use crate::utils::is_number_token_char;
use anyhow::Result;
use std::iter::{Enumerate, Peekable};
use std::str::Chars;

pub struct Lexer<'a> {
    input: Peekable<Enumerate<Chars<'a>>>,
}

impl<'a> Lexer<'a> {
    pub fn new(input: &'a str) -> Self {
        Lexer {
            input: input.chars().enumerate().peekable(),
        }
    }

    pub fn tokenize(&mut self) -> Result<Vec<Token>> {
        let mut tokens = vec![];

        while let Some((index, c)) = self.input.next() {
            match c {
                '{' => tokens.push(Token::OpenBrace),
                '}' => tokens.push(Token::CloseBrace),
                '[' => tokens.push(Token::OpenBracket),
                ']' => tokens.push(Token::CloseBracket),
                '"' => {
                    let token = self.scan_string_token()?;
                    tokens.push(token);
                }
                c if is_number_token_char(c) => {
                    let token = self.scan_number_token(c)?;
                    tokens.push(token);
                }
                't' => {
                    let token = self.scan_bool_token(true, index)?;
                    tokens.push(token);
                }
                'f' => {
                    let token = self.scan_bool_token(false, index)?;
                    tokens.push(token);
                }
                'n' => {
                    let token = self.scan_null_token(index)?;
                    tokens.push(token);
                }
                ':' => tokens.push(Token::Colon),
                ',' => tokens.push(Token::Comma),
                '/' => {
                    let token = self.scan_comment_token()?;
                    tokens.push(token);
                }
                ' ' => {
                    let token = self.scan_whitespaces()?;
                    tokens.push(token);
                }
                '\n' => tokens.push(Token::BreakLine),
                _ => (),
            };
        }

        Ok(tokens)
    }

    fn scan_string_token(&mut self) -> Result<Token> {
        let mut value = String::new();

        while let Some((_index, c)) = self.input.next() {
            match c {
                '"' => {
                    return Ok(Token::StringValue(value));
                }
                '\\' => {
                    let (_, c2) = self
                        .input
                        .next()
                        .ok_or(LexerError::NotExistTerminalSymbol)?;
                    match c2 {
                        'u' => {
                            let hex = self.take_chars_with(4);
                            if hex.len() != 4 && hex.parse::<f64>().is_ok() {
                                return Err(LexerError::NotExistTerminalSymbol.into());
                            }

                            value.push_str(&format!("\\u{}", hex));
                        }
                        '"' | '\\' | '/' | 'b' | 'f' | 'n' | 'r' | 't' => {
                            value.push_str(&format!("\\{}", c2));
                        }
                        _ => {
                            return Err(LexerError::NotEscapeString.into());
                        }
                    }
                }
                _ => {
                    value.push(c);
                }
            }
        }
        Err(LexerError::NotExistTerminalSymbol.into())
    }

    fn scan_number_token(&mut self, first: char) -> Result<Token> {
        let mut value = String::new();
        value.push(first);

        while let Some((_index, c)) = self.input.peek() {
            if is_number_token_char(*c) {
                let (_, c) = self.input.next().unwrap();
                value.push(c);
            } else {
                return Ok(Token::Number(value));
            }
        }
        Err(LexerError::NotExistTerminalSymbol.into())
    }

    fn scan_bool_token(&mut self, expect_bool: bool, index: usize) -> Result<Token> {
        let s: String;
        let (s, end) = if expect_bool {
            // ã™ã§ã«æœ€åˆã®`t`ã¯æ¶ˆè²»ã•ã‚Œã¦ã„ã‚‹å‰æãªã®ã§æ®‹ã‚Šæ–‡å­—ã‚’ç²¾æŸ»
            s = "t".to_string() + &self.take_chars_with(3);
            (s, index + 3)
        } else {
            // ã™ã§ã«æœ€åˆã®`f`ã¯æ¶ˆè²»ã•ã‚Œã¦ã„ã‚‹å‰æãªã®ã§æ®‹ã‚Šæ–‡å­—ã‚’ç²¾æŸ»
            s = "f".to_string() + &self.take_chars_with(4);
            (s, index + 4)
        };
        let location = Location(index, end);
        match &s as &str {
            "true" => Ok(Token::Boolean(true)),
            "false" => Ok(Token::Boolean(false)),
            other => Err(LexerError::InvalidChars(other.to_string(), location).into()),
        }
    }

    fn scan_null_token(&mut self, index: usize) -> Result<Token> {
        // `null`ã‹ã©ã†ã‹æ–‡å­—ã‚’å–å¾—
        let s = "n".to_string() + &self.take_chars_with(3);
        let location = Location(index, index + 3);
        if s == "null" {
            Ok(Token::Null)
        } else {
            Err(LexerError::InvalidChars(s.to_string(), location).into())
        }
    }

    fn scan_comment_token(&mut self) -> Result<Token> {
        let (second_slash, next_char) = self
            .input
            .next()
            .ok_or(LexerError::NotExistTerminalSymbol)?;
        match next_char {
            '/' => {
                let mut value = String::new();
                while let Some((_index, c)) = self.input.peek() {
                    if c == &'\n' {
                        return Ok(Token::CommentLine(value));
                    } else {
                        // peekã—ã¦ã‚‹ã®ã§unwrap
                        let (_, c) = self.input.next().unwrap();
                        value.push(c);
                    }
                }
            }
            '*' => {
                let mut value = String::new();
                let mut asterisk_buffer = String::new();
                let mut prev_asterisk = false;
                while let Some((_index, c)) = self.input.next() {
                    match c {
                        '*' => {
                            prev_asterisk = true;
                            asterisk_buffer.push(c);
                        }
                        '/' => {
                            if prev_asterisk {
                                return Ok(Token::CommentBlock(value));
                            }
                        }
                        _ => {
                            if prev_asterisk {
                                value.push_str(&asterisk_buffer);
                                asterisk_buffer.clear();
                            }
                            prev_asterisk = false;
                            value.push(c);
                        }
                    };
                }
            }
            c => {
                return Err(LexerError::InvalidChars(
                    format!("/{}", c).to_string(),
                    Location(second_slash, second_slash + 1),
                )
                .into())
            }
        }
        Err(LexerError::NotExistTerminalSymbol.into())
    }

    fn scan_whitespaces(&mut self) -> Result<Token> {
        let mut length: usize = 1; // å‘¼ã³å‡ºã—æ™‚ç‚¹ã§1
        while let Some((_index, c)) = self.input.peek() {
            let c = *c;
            match c {
                ' ' => {
                    self.input.next().unwrap();
                    length += 1
                }
                _ => {
                    return Ok(Token::WhiteSpaces(length as i32));
                }
            }
        }
        Err(LexerError::NotExistTerminalSymbol.into())
    }

    fn take_chars_with(&mut self, times: i32) -> String {
        let chars = (0..times)
            .filter_map(|_| self.input.next().map(|(_index, c)| c))
            .collect::<String>();
        chars
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::token::Token;

    #[test]
    fn lexer_should_success_scan() {
        let mut lexer = Lexer::new(
            r#"{
    "name": "sato",
    "age": 20,
    "flag": false,
    "attr": null
    // line
    /**
     * block
     */
}"#,
        );
        let result = lexer.tokenize().expect("lexerã¯é…åˆ—ã‚’è¿”ã—ã¾ã™ã€‚");
        let expected = [
            Token::OpenBrace,
            Token::BreakLine,
            Token::WhiteSpaces(4),
            Token::StringValue("name".to_string()),
            Token::Colon,
            Token::WhiteSpaces(1),
            Token::StringValue("sato".to_string()),
            Token::Comma,
            Token::BreakLine,
            Token::WhiteSpaces(4),
            Token::StringValue("age".to_string()),
            Token::Colon,
            Token::WhiteSpaces(1),
            Token::Number("20".to_string()),
            Token::Comma,
            Token::BreakLine,
            Token::WhiteSpaces(4),
            Token::StringValue("flag".to_string()),
            Token::Colon,
            Token::WhiteSpaces(1),
            Token::Boolean(false),
            Token::Comma,
            Token::BreakLine,
            Token::WhiteSpaces(4),
            Token::StringValue("attr".to_string()),
            Token::Colon,
            Token::WhiteSpaces(1),
            Token::Null,
            Token::BreakLine,
            Token::WhiteSpaces(4),
            Token::CommentLine(" line".to_string()),
            Token::BreakLine,
            Token::WhiteSpaces(4),
            Token::CommentBlock(
                r#"*
     * block
     "#
                .to_string(),
            ),
            Token::BreakLine,
            Token::CloseBrace,
        ];
        for (index, expect) in expected.iter().enumerate() {
            assert_eq!(expect, &result[index], "tokenã®{}ç•ªç›®ãŒæƒ³å®šå¤–ã§ã™ã€‚", index,);
        }
        assert_eq!(36, result.len(), "tokené…åˆ—é•·ãŒæƒ³å®šå¤–ã§ã™ã€‚");
    }

    #[test]
    fn scan_string_token_should_return_token() {
        let mut lexer = Lexer::new(r#""name123""#);
        // æœ€åˆã®"ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let token = lexer
            .scan_string_token()
            .expect("[scan_string_token_should_return_token]\"name\"ã®scanã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
        assert_eq!(Token::StringValue("name123".to_string()), token);

        let mut lexer = Lexer::new(r#""ã‚ã„ã†ãˆãŠ""#);
        // æœ€åˆã®"ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let token = lexer
            .scan_string_token()
            .expect("[scan_string_token_should_return_token]\"ã‚ã„ã†ãˆãŠ\"ã®scanã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
        assert_eq!(Token::StringValue("ã‚ã„ã†ãˆãŠ".to_string()), token);

        let mut lexer = Lexer::new(r#""\u3042\u3044\u3046abc""#);
        // æœ€åˆã®"ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let token = lexer
            .scan_string_token()
            .expect("[scan_string_token_should_return_token]\"ã‚ã„ã†abc\"ã®scanã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
        assert_eq!(
            Token::StringValue("\\u3042\\u3044\\u3046abc".to_string()),
            token
        );

        let mut lexer = Lexer::new(r#""\ud83d\ude00\ud83d\udc4d""#);
        // æœ€åˆã®"ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let token = lexer
            .scan_string_token()
            .expect("[scan_string_token_should_return_token]\"ğŸ˜€ğŸ‘\"ã®scanã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
        assert_eq!(
            Token::StringValue("\\ud83d\\ude00\\ud83d\\udc4d".to_string()),
            token
        );

        let mut lexer = Lexer::new(r#""ğŸ˜€ğŸ‘""#);
        // æœ€åˆã®"ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let token = lexer
            .scan_string_token()
            .expect("[scan_string_token_should_return_token]\"ğŸ˜€ğŸ‘\"ã®scanã«å¤±æ•—ã—ã¾ã—ãŸã€‚");
        assert_eq!(Token::StringValue("ğŸ˜€ğŸ‘".to_string()), token);

        let mut lexer = Lexer::new(r#""test\"\/\\\b\n\f\r\t""#);
        // æœ€åˆã®"ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let token = lexer
            .scan_string_token()
            .expect(r#"[scan_string_token_should_return_token]"test\"\/\\\b\n\f\r\t""ã®scanã«å¤±æ•—ã—ã¾ã—ãŸã€‚"#);
        assert_eq!(
            Token::StringValue(r#"test\"\/\\\b\n\f\r\t"#.to_string()),
            token
        );
    }

    #[test]
    fn scan_string_token_should_err() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new("name");
        assert!(lexer.scan_string_token().is_err());
    }

    #[test]
    fn scan_number_token_should_return_token() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":100,");
        // æœ€åˆã®`"`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (_, first) = lexer.input.next().unwrap();
        if let Ok(token) = lexer.scan_number_token(first) {
            assert_eq!(Token::Number("100".to_string()), token);
        } else {
            panic!("[scan_string_token]ãŒErrã‚’è¿”ã—ã¾ã—ãŸã€‚");
        };
    }

    #[test]
    fn scan_number_token_should_err() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":100");
        // æœ€åˆã®`"`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (_, first) = lexer.input.next().unwrap();
        assert!(lexer.scan_number_token(first).is_err());
    }

    #[test]
    fn scan_bool_token_should_return_true_token() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":true,");
        // æœ€åˆã®`t`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (index, _) = lexer.input.next().unwrap();
        if let Ok(token) = lexer.scan_bool_token(true, index) {
            assert_eq!(Token::Boolean(true), token);
        } else {
            panic!("[scan_string_token]ãŒErrã‚’è¿”ã—ã¾ã—ãŸã€‚");
        };
    }

    #[test]
    fn scan_bool_token_should_err_with_true() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":tru");
        // æœ€åˆã®`t`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (index, _) = lexer.input.next().unwrap();
        assert!(lexer.scan_bool_token(true, index).is_err());
    }

    #[test]
    fn scan_bool_token_should_return_false_token() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":false,");
        // æœ€åˆã®`f`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (index, _) = lexer.input.next().unwrap();
        if let Ok(token) = lexer.scan_bool_token(false, index) {
            assert_eq!(Token::Boolean(false), token);
        } else {
            panic!("[scan_bool_token]ãŒErrã‚’è¿”ã—ã¾ã—ãŸã€‚");
        };
    }

    #[test]
    fn scan_bool_token_should_err_with_false() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":fal");
        // æœ€åˆã®`f`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (index, _) = lexer.input.next().unwrap();
        assert!(lexer.scan_bool_token(false, index).is_err());
    }

    #[test]
    fn scan_null_token_should_return_token() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":null,");
        // æœ€åˆã®`f`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (index, _) = lexer.input.next().unwrap();
        if let Ok(token) = lexer.scan_null_token(index) {
            assert_eq!(Token::Null, token);
        } else {
            panic!("[scan_null_token]ãŒErrã‚’è¿”ã—ã¾ã—ãŸã€‚");
        };
    }

    #[test]
    fn scan_null_token_should_err() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(":nu");
        // æœ€åˆã®`n`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        let (index, _) = lexer.input.next().unwrap();
        assert!(lexer.scan_null_token(index).is_err());
    }

    #[test]
    fn scan_comment_line_token_should_return_token() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(",// comment \n}");
        // æœ€åˆã®`/`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        lexer.input.next();
        if let Ok(token) = lexer.scan_comment_token() {
            assert_eq!(Token::CommentLine(" comment ".to_string()), token);
        } else {
            panic!("[scan_comment_token]ãŒErrã‚’è¿”ã—ã¾ã—ãŸã€‚");
        };
    }

    #[test]
    fn scan_comment_block_token_should_return_token() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(
            r#"/*
**
test comment
**
*/"#,
        );
        // æœ€åˆã®`/`ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        if let Ok(token) = lexer.scan_comment_token() {
            assert_eq!(
                Token::CommentBlock(
                    r#"
**
test comment
**
"#
                    .to_string()
                ),
                token
            );
        } else {
            panic!("[scan_comment_token]ãŒErrã‚’è¿”ã—ã¾ã—ãŸã€‚");
        };
    }

    #[test]
    fn scan_comment_token_should_err() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new("/,");
        lexer.input.next().unwrap();
        assert!(lexer.scan_comment_token().is_err());
    }

    #[test]
    fn scan_whitespaces_token_should_return_token() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(r#"   ""#);
        // æœ€åˆã®` `ã¾ã§é€²ã‚ã‚‹
        lexer.input.next();
        if let Ok(token) = lexer.scan_whitespaces() {
            assert_eq!(Token::WhiteSpaces(3), token);
        } else {
            panic!("[scan_whitespaces]ãŒErrã‚’è¿”ã—ã¾ã—ãŸã€‚");
        };
    }

    #[test]
    fn scan_whitespaces_token_should_err() {
        // éƒ¨åˆ†çš„ãªãƒ†ã‚¹ãƒˆã®ãŸã‚ã®invalid json
        let mut lexer = Lexer::new(r#"  "#);
        lexer.input.next().unwrap();
        assert!(lexer.scan_whitespaces().is_err());
    }
}
